# -*- coding: utf-8 -*-
"""Tool Pipeline Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M6bX72-UMPo1RDpxOaWPxE6KvDTqrUtu

# **Retrieve Documents via API Call**
"""

!pip install feedparser

"""Walmart Legal Document Retrieval"""

import requests
import feedparser

headers = {"User-Agent": "Michelle Ike (mike2@student.gsu.edu)"}
CIK = "0000104169"
feed_url = f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={CIK}&owner=exclude&count=5&output=atom"
response = requests.get(feed_url, headers=headers)

feed = feedparser.parse(response.text)
for entry in feed.entries:
    print("Title:", entry.title)
    print("Filing Detail Page:", entry.link)
    print("---")

"""Scraping Exhibit Links from Filing Page"""

pip install beautifulsoup4 lxml

import requests
from bs4 import BeautifulSoup

def get_main_filing_document(index_url):
    headers = {
        "User-Agent": "Michelle Ike (mike2@student.gsu.edu)"
    }

    print(f"üîç Fetching filing index: {index_url}")
    response = requests.get(index_url, headers=headers)

    if response.status_code != 200:
        print(f"‚ùå Failed to fetch page. Status code: {response.status_code}")
        return None

    soup = BeautifulSoup(response.content, "lxml")

    # Find the main document table
    table = soup.find("table", class_="tableFile", summary="Document Format Files")
    if not table:
        print("‚ö†Ô∏è Could not find document table.")
        return None

    print("‚úÖ Document table found.")

    rows = table.find_all("tr")[1:]  # Skip header row
    for row in rows:
        cols = row.find_all("td")
        if len(cols) >= 4:
            doc_desc = cols[3].text.strip().lower()
            doc_name = cols[2].text.strip()
            if "10-k" in doc_desc or "complete submission text file" in doc_desc:
                href = cols[2].find("a")["href"]
                full_link = "https://www.sec.gov" + href
                print(f"üìÑ Found filing document: {doc_name} ‚Äì {full_link}")
                return full_link

    print("‚ö†Ô∏è No main document (10-K or full text) found.")
    return None

index_url = "https://www.sec.gov/Archives/edgar/data/104169/000010416925000021/0000104169-25-000021-index.htm"

doc_url = get_main_filing_document(index_url)

if doc_url:
    print("üéØ Document URL:", doc_url)
else:
    print("‚ùå No filing document extracted.")

def extract_filing_text(document_url):
    headers = {
        "User-Agent": "Michelle Ike (mike2@student.gsu.edu)"
    }

    print(f"üìÑ Downloading filing: {document_url}")
    response = requests.get(document_url, headers=headers)

    if response.status_code != 200:
        print(f"‚ùå Failed to retrieve document. Status: {response.status_code}")
        return ""

    soup = BeautifulSoup(response.content, "html.parser")
    text = soup.get_text(separator=" ", strip=True)

    print(f"‚úÖ Extracted {len(text)} characters from document.")
    return text

# Clean version of the document URL (strip ix?doc=)
raw_url = doc_url.replace("https://www.sec.gov/ix?doc=", "https://www.sec.gov")

filing_text = extract_filing_text(raw_url)

print(filing_text[:1000])  # Preview the first 1000 characters

"""# Text Preprocessing: Cleaning and Structuring the text"""

import re

def clean_filing_text(text):
    # Remove XBRL-like tags (e.g., wmt:something)
    text = re.sub(r'\bwmt:[\w\d]+', '', text)
    text = re.sub(r'\bxbrli:[\w\d]+', '', text)
    text = re.sub(r'\biso4217:[A-Z]+', '', text)

    # Remove repeated CIK/ISIN-style sequences
    text = re.sub(r'\b\d{10}\b', '', text)

    # Collapse multiple spaces
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

def chunk_text(text, max_tokens=300):
    from nltk.tokenize import sent_tokenize
    import nltk
    nltk.download("punkt")

    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = []

    for sentence in sentences:
        current_chunk.append(sentence)
        if sum(len(s.split()) for s in current_chunk) > max_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = []

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

!pip install bertopic
!pip install sentence_transformers

from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import nltk
nltk.download("punkt")
nltk.download("punkt_tab")  # This one resolves the LookupError

model = SentenceTransformer("all-MiniLM-L6-v2")
topic_model = BERTopic()

cleaned = clean_filing_text(filing_text)
chunks = chunk_text(cleaned)

embeddings = model.encode(chunks, show_progress_bar=True)
topics, probs = topic_model.fit_transform(chunks, embeddings)

topic_model.get_topic_info()

import json
import os

def save_topics_to_drive(topic_model, chunks, topics, filename="walmart_10k_topics.json"):
    drive_path = "/content/drive/My Drive/GenAI/BertTopicOutputs"
    os.makedirs(drive_path, exist_ok=True)

    output = []
    for i, topic_id in enumerate(topics):
        if topic_id != -1:
            topic_info = topic_model.get_topic(topic_id)
            keywords = [kw for kw, _ in topic_info]
            output.append({
                "chunk_index": i,
                "topic_id": topic_id,
                "chunk_text": chunks[i],
                "keywords": keywords
            })

    full_path = os.path.join(drive_path, filename)
    with open(full_path, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2)

    print(f"‚úÖ Saved {len(output)} topic chunks to {full_path}")

save_topics_to_drive(topic_model, chunks, topics)

"""# Creating a fetching, cleaning and creation of embeddings pipeline for all documents"""

#optimized code to fetch, clean and create embeddings for each document, saving the outputs for 10 other companies.
def process_company_filings(
    company_name,
    cik,
    form_type="10-K",
    filings_to_fetch=2,
    save_dir="/content/drive/My Drive/LegalAI/BERTopicOutputs"
):
    import os
    import json
    import re
    import requests
    import feedparser
    from bs4 import BeautifulSoup
    from nltk.tokenize import sent_tokenize
    from bertopic import BERTopic
    from sentence_transformers import SentenceTransformer

    headers = {
        "User-Agent": "Michelle Ike (mike2@student.gsu.edu)"
    }

    feed_url = f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={form_type}&owner=exclude&count={filings_to_fetch}&output=atom"
    feed = feedparser.parse(requests.get(feed_url, headers=headers).text)
    index_urls = [entry.link for entry in feed.entries]

    def get_main_doc(index_url):
        soup = BeautifulSoup(requests.get(index_url, headers=headers).content, "lxml")
        table = soup.find("table", class_="tableFile", summary="Document Format Files")
        for row in table.find_all("tr")[1:]:
            cols = row.find_all("td")
            if len(cols) >= 4 and ("10-k" in cols[3].text.lower() or "complete" in cols[3].text.lower()):
                return "https://www.sec.gov" + cols[2].find("a")["href"]
        return None

    def extract_text(url):
        html = requests.get(url, headers=headers).content
        soup = BeautifulSoup(html, "html.parser")
        return soup.get_text(separator=" ", strip=True)

    def clean_text(text):
        text = re.sub(r'\bwmt:[\w\d]+', '', text)
        text = re.sub(r'\bxbrli:[\w\d]+', '', text)
        text = re.sub(r'\biso4217:[A-Z]+', '', text)
        text = re.sub(r'\b\d{10}\b', '', text)
        return re.sub(r'\s+', ' ', text).strip()

    def chunk(text, max_tokens=300):
        sentences = sent_tokenize(text)
        chunks, current, length = [], [], 0
        for sentence in sentences:
            tokens = len(sentence.split())
            if length + tokens > max_tokens:
                chunks.append(" ".join(current))
                current, length = [], 0
            current.append(sentence)
            length += tokens
        if current:
            chunks.append(" ".join(current))
        return chunks

    model = SentenceTransformer("all-MiniLM-L6-v2")
    topic_model = BERTopic()
    os.makedirs(save_dir, exist_ok=True)

    for i, index_url in enumerate(index_urls):
        try:
            main_url = get_main_doc(index_url)
            if not main_url:
                print(f"‚ö†Ô∏è No filing document found for {index_url}")
                continue

            raw = extract_text(main_url)
            cleaned = clean_text(raw)
            chunks = chunk(cleaned)

            if len(chunks) < 2:
                print(f"‚ö†Ô∏è Skipping {company_name} filing #{i+1}: Not enough chunks for BERTopic.")
                continue

            embeddings = model.encode(chunks, show_progress_bar=False)
            topics, _ = topic_model.fit_transform(chunks, embeddings)

            output = []
            for j, topic_id in enumerate(topics):
                if topic_id != -1:
                    keywords = [kw for kw, _ in topic_model.get_topic(topic_id)]
                    output.append({
                        "company": company_name,
                        "filing_url": main_url,
                        "chunk_index": j,
                        "topic_id": topic_id,
                        "chunk_text": chunks[j],
                        "keywords": keywords
                    })

            save_path = os.path.join(save_dir, f"{company_name}_{i+1}_{form_type}_topics.json")
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(output, f, indent=2)
            print(f"‚úÖ Saved {len(output)} topics to {save_path}")
        except Exception as e:
            print(f"‚ùå Failed to process filing #{i+1} for {company_name}: {e}")

def batch_process_companies(company_cik_map, filings_to_fetch=2, form_type="10-K"):
    """
    Run the clause extraction and BERTopic pipeline for multiple companies.

    Parameters:
        company_cik_map (dict): Dictionary of {"CompanyName": "CIK"}
        filings_to_fetch (int): Number of recent filings to fetch per company
        form_type (str): Type of SEC filing (e.g., "10-K", "8-K", "S-1")
    """
    all_status = []
    for name, cik in company_cik_map.items():
        try:
            print(f"\nüöÄ Processing: {name} ({form_type})")
            process_company_filings(
                company_name=name,
                cik=cik,
                form_type=form_type,
                filings_to_fetch=filings_to_fetch,
                save_dir="/content/drive/My Drive/GenAI/BertTopicOutputs"
            )
            all_status.append({"Company": name, "Status": "‚úÖ Success"})
        except Exception as e:
            print(f"‚ùå Error with {name}: {e}")
            all_status.append({"Company": name, "Status": f"‚ùå Failed: {str(e)}"})
    return all_status
company_cik_map = {
    "Microsoft": "0000789019",
    "JPMorganChase": "0000019617",
    "JohnsonAndJohnson": "0000200406",
    "ExxonMobil": "0000034088",
    "Netflix": "0001065280",
    "Ford": "0000037996",
    "3M": "0000066740",
    "DeltaAirLines": "0000027904",
    "ATT": "0000732717",
    "FedEx": "0001048911"
}

# Run it
batch_results = batch_process_companies(company_cik_map, filings_to_fetch=2)

"""# Labeling

**Objective:**

To generate high-quality "ground truth" clause labels for training a legal clause classification model, we manually labeled a chunked contract dataset using an AI-assisted approach.

**Approach:**

We used OpenAI's GPT-4-turbo model as a legal assistant to help label paragraphs (chunks) from contract documents.
Each chunk was assessed for its semantic meaning, and if relevant, labeled with one of the pre-defined legal clause types.
Where no clause applied, the label "none" was assigned.

**What unique labels are present in the dataset?**
"""

import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/GenAI/labeled_contract_chunks.csv")

# Clean labels
df_clean = df.dropna(subset=["manual_label"])
df_clean = df_clean[df_clean["manual_label"].str.lower() != "none"]

# Count and plot unique labels
label_counts = df_clean["manual_label"].value_counts()

plt.figure(figsize=(10, 6))
label_counts.plot(kind="barh")
plt.title(" Distribution of Clause Labels (manual_label)")
plt.xlabel("Count")
plt.ylabel("Clause Type")
plt.gca().invert_yaxis()
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""# Creating a Legal Clause Classifier
**Exploring Logistic Regression, SVM, XGBoost and Random Forest**

SMOTE + Logistic Regression (Legal Text Classifier) - *Morgan*
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q imbalanced-learn
!pip install -q transformers datasets

# üìö Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, f1_score
from imblearn.over_sampling import SMOTE

# Load data
file_path = "/content/drive/MyDrive/GenAI/labeled_contract_chunks.csv"
df = pd.read_csv(file_path)

# Filter out 'none' and nulls in text or labels
df_clean = df.dropna(subset=["text", "manual_label"]).copy()
df_clean = df_clean[df_clean["manual_label"].str.lower() != "none"]

#  TF-IDF vectorization on clause text
X = df_clean["text"]
y = df_clean["manual_label"]
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))
X_vec = vectorizer.fit_transform(X)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42, stratify=y)

#  Balance classes with SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Train Logistic Regression
clf = LogisticRegression(class_weight="balanced", max_iter=1000)
clf.fit(X_resampled, y_resampled)

# Evaluate model
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average="macro")
f1_micro = f1_score(y_test, y_pred, average="micro")
f1_weighted = f1_score(y_test, y_pred, average="weighted")

# üßæ Full report
print(classification_report(y_test, y_pred))
print(f"\n Accuracy:          {accuracy:.4f}")
print(f" Macro F1-score:    {f1_macro:.4f}")
print(f" Micro F1-score:    {f1_micro:.4f}")
print(f"Weighted F1-score: {f1_weighted:.4f}")

"""SVM (Support Vector Machine)"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Load labeled data
df = pd.read_csv("/content/drive/MyDrive/GenAI/labeled_contract_chunks.csv")

df_labeled = df[df["manual_label"].str.lower() != "none"].copy()
print(f" Remaining labeled samples: {len(df_labeled)}")

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

X = df_labeled["text"]
y = df_labeled["manual_label"]

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Vectorize
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score, f1_score

# Train SVM

svm = LinearSVC(
    class_weight='balanced',
    random_state=42
)

svm.fit(X_train_vec, y_train)
y_pred_svm = svm.predict(X_test_vec)

print("üìä SVM Results")
print(classification_report(y_test, y_pred_svm))
print(f"\n Accuracy:          {accuracy:.4f}")
print(f" Macro F1-score:    {f1_macro:.4f}")
print(f" Micro F1-score:    {f1_micro:.4f}")
print(f"Weighted F1-score: {f1_weighted:.4f}")

"""XGBoost"""

from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, f1_score


# Encode labels
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

xgb = XGBClassifier(
    objective='multi:softmax',
    num_class=len(y.unique()),  # or len(le.classes_) if label-encoded
    eval_metric='mlogloss',
    use_label_encoder=False,
    random_state=42
)
xgb.fit(X_train_vec, y_train_enc)
y_pred_xgb = xgb.predict(X_test_vec)
accuracy = accuracy_score(y_test_enc, y_pred_xgb)
f1_macro = f1_score(y_test_enc, y_pred_xgb, average="macro")
f1_micro = f1_score(y_test_enc, y_pred_xgb, average="micro")
f1_weighted = f1_score(y_test_enc, y_pred_xgb, average="weighted")


print("üìä XGBoost Results")
print(classification_report(y_test_enc, y_pred_xgb, target_names=le.classes_))
print(f"\n Accuracy:          {accuracy:.4f}")
print(f" Macro F1-score:    {f1_macro:.4f}")
print(f" Micro F1-score:    {f1_micro:.4f}")
print(f"Weighted F1-score: {f1_weighted:.4f}")

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBClassifier

joblib.dump(xgb, "xgboost_model.pkl")
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")
joblib.dump(le, "label_encoder.pkl")

"""Random Forest"""

# Imports
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score, f1_score

# Load labeled data
df = pd.read_csv("/content/drive/MyDrive/GenAI/labeled_contract_chunks.csv")

# Clean: remove 'none' and nulls
df_clean = df.dropna(subset=["text", "manual_label"])
df_clean = df_clean[df_clean["manual_label"].str.lower() != "none"]

# Define features and labels
X = df_clean["text"]
y = df_clean["manual_label"]

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# TF-IDF vectorization
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Random Forest model
rf_clf = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42)
rf_clf.fit(X_train_vec, y_train)

# Predict
y_pred_rf = rf_clf.predict(X_test_vec)

# Evaluation
print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

# Optional: Print overall F1 scores
print(f"\n Accuracy:         {accuracy_score(y_test, y_pred_rf):.4f}")
print(f" Macro F1 Score:   {f1_score(y_test, y_pred_rf, average='macro'):.4f}")
print(f" Weighted F1 Score:{f1_score(y_test, y_pred_rf, average='weighted'):.4f}")

"""Cross Validation"""

from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
import pandas as pd

# Load & clean
df = pd.read_csv("/content/drive/MyDrive/GenAI/labeled_contract_chunks.csv")
df_clean = df.dropna(subset=["text", "manual_label"])
df_clean = df_clean[df_clean["manual_label"].str.lower() != "none"]
X = df_clean["text"]
y = df_clean["manual_label"]

# Cross-validation setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "SVM": LinearSVC(class_weight='balanced'),
    "XGBoost": XGBClassifier(eval_metric='mlogloss'),
    "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42)
}

#  Store results
results = {}

#  Loop through models
for name, model in models.items():
    print(f" Cross-validating {name}...")

    # For XGBoost, encode labels
    if name == "XGBoost":
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        y_used = y_encoded
    else:
        y_used = y  # use raw string labels

    # Build pipeline
    pipeline = Pipeline([
        ("tfidf", TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
        ("clf", model)
    ])

    # Run CV
    scores = cross_val_score(pipeline, X, y_used, cv=cv, scoring="f1_macro")
    results[name] = scores
    print(f"{name} Macro F1 (avg): {scores.mean():.4f} ¬± {scores.std():.4f}\n")

# Summary
results_df = pd.DataFrame(results)
print(results_df.mean().to_frame("Mean Macro F1"))

"""Model Comparison"""

import matplotlib.pyplot as plt

# F1 scores
model_names = ["Logistic Regression", "SVM", "XGBoost","Random Forest"]
mean_f1_scores = [0.6444, 0.7583, 0.7837,0.6728 ]

# Plot
plt.figure(figsize=(8, 5))
bars = plt.bar(model_names, mean_f1_scores, color=["#6cace4", "#fdb913", "#00a550", "#FF6F61"])
plt.ylim(0.5, 0.85)
plt.title("Cross-Validated Mean Macro F1 Score by Model")
plt.ylabel("Macro F1 Score")

# Add text on top of bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, height + 0.01,
             f"{height:.4f}", ha='center', va='bottom')

plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

"""XGBoost is the best performing model

# Integrating Summarization
"""

!pip install pdfplumber

!pip install openai==0.28

import openai
print(openai.__version__)

# üìö Imports
import os
import openai
import pandas as pd
import pdfplumber
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from google.colab import drive

# üöÄ Step 1: Mount Google Drive
drive.mount('/content/drive')

# üîê Step 2: Read API Key from file
key_path = '/content/drive/MyDrive/GenAI/openai_key.txt'
with open(key_path, 'r') as file:
    openai_api_key = file.read().strip()

openai.api_key = openai_api_key

# üõ† Step 3: Load Trained Models
xgb_model = joblib.load("xgboost_model.pkl")
vectorizer = joblib.load("tfidf_vectorizer.pkl")
label_encoder = joblib.load("label_encoder.pkl")

# üìÑ Step 4: Extract Full Text from Filing
def extract_text_from_pdf(pdf_path):
    """Extracts clean text from all pages in a PDF."""
    all_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                all_text += text + "\n\n"
    return all_text

# üìÇ Load Filing
pdf_path = "Confidentiality_Agreement.pdf"  # Adjust path if needed
full_text = extract_text_from_pdf(pdf_path)

# ‚úÇÔ∏è Step 5: Split Full Text into Chunks
chunks = full_text.split("\n\n")

# üß† Step 6: Vectorize Chunks
X_chunks = vectorizer.transform(chunks)

# üîç Step 7: Predict Clause Types
y_pred = xgb_model.predict(X_chunks)
predicted_clauses = label_encoder.inverse_transform(y_pred)

# üóÇÔ∏è Step 8: Organize by Clause
classified_chunks = pd.DataFrame({
    "chunk": chunks,
    "predicted_clause": predicted_clauses
})

grouped = classified_chunks.groupby("predicted_clause")["chunk"].apply(lambda x: " ".join(x)).to_dict()

# ‚ú® Step 9: Define Summarization Function
def summarize_text(text, clause_type):
    prompt = f"""
You are an expert legal summarization assistant.
Please summarize the following text extracted from a "{clause_type}" section of a legal document.
Summarize professionally in 3-5 sentences focusing on key points.

Text:
\"\"\"
{text}
\"\"\"

Summary:
    """
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful legal summarizer."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2,
        max_tokens=500
    )
    return response["choices"][0]["message"]["content"].strip()

# üìö Step 10: Summarize Each Grouped Clause
structured_summaries = {}

for clause_type, text in grouped.items():
    if len(text) > 5000:
        text = text[:5000]  # Optional: truncate if too large
    summary = summarize_text(text, clause_type)
    structured_summaries[clause_type] = summary

# üìã Step 11: Assemble Final Structured Summary
for clause, summary in structured_summaries.items():
    print(f"\nüóÇÔ∏è {clause}")
    print(summary)

